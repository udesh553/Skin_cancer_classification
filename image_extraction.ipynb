{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":104884,"sourceType":"datasetVersion","datasetId":54339}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.utils import resample\nimport matplotlib.pyplot as plt\nimport math\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image,ImageOps\nfrom cv2 import imread, resize,cvtColor, imwrite\nfrom os.path import exists\nfrom scipy import ndimage\nfrom skimage.measure import label, regionprops\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n \nfrom IPython.display import display\nimport os\n\nimport cv2\n \nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n \nfrom tensorflow.keras.utils import to_categorical\nimport itertools\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\n \nimport numpy as np, pandas as pd\n \n \n ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-04T15:24:08.979362Z","iopub.execute_input":"2023-12-04T15:24:08.980032Z","iopub.status.idle":"2023-12-04T15:24:08.992931Z","shell.execute_reply.started":"2023-12-04T15:24:08.979980Z","shell.execute_reply":"2023-12-04T15:24:08.991885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_skin_dir = os.path.join('..', 'input')\n\n# Merging images from both folders HAM10000_images_part1.zip and HAM10000_images_part2.zip into one dictionary\n\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x\n                     for x in glob(os.path.join(base_skin_dir,\"skin-cancer-mnist-ham10000/\", '*', '*.jpg'))}\nlesion_type_dict = {\n    'nv': 'Melanocytic nevi (nv)',\n    'mel': 'Melanoma (mel)',\n    'bkl': 'Benign keratosis-like lesions (bkl)',\n    'bcc': 'Basal cell carcinoma (bcc)',\n    'akiec': 'Actinic keratoses (akiec)',\n    'vasc': 'Vascular lesions (vasc)',\n    'df': 'Dermatofibroma (df)'\n}\nlabel_mapping = {\n    0: 'nv',\n    1: 'mel',\n    2: 'bkl',\n    3: 'bcc',\n    4: 'akiec',\n    5: 'vasc',\n    6: 'df'\n}\nreverse_label_mapping = dict((value, key) for key, value in label_mapping.items())","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:09.169642Z","iopub.execute_input":"2023-12-04T15:24:09.170664Z","iopub.status.idle":"2023-12-04T15:24:10.209320Z","shell.execute_reply.started":"2023-12-04T15:24:09.170611Z","shell.execute_reply":"2023-12-04T15:24:10.208276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"paths=[]\nids=[]\nfor dirname, _, filenames in os.walk('/kaggle/input/ham10000-lesion-segmentations/HAM10000_segmentations_lesion_tschandl'):\n    for filename in filenames:\n        ids.append(filename.split(\"_seg\")[0])\n        paths.append(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:10.211919Z","iopub.execute_input":"2023-12-04T15:24:10.212780Z","iopub.status.idle":"2023-12-04T15:24:10.220951Z","shell.execute_reply.started":"2023-12-04T15:24:10.212731Z","shell.execute_reply":"2023-12-04T15:24:10.219479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_segment(img_id):\n    contains_abc = segment_df['image_id'].str.contains(img_id)\n    return segment_df[contains_abc].segment_path.values[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:10.224304Z","iopub.execute_input":"2023-12-04T15:24:10.225294Z","iopub.status.idle":"2023-12-04T15:24:10.237549Z","shell.execute_reply.started":"2023-12-04T15:24:10.225241Z","shell.execute_reply":"2023-12-04T15:24:10.236591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_skin= pd.read_csv(\"/kaggle/input/skin-cancer-mnist-ham10000/HAM10000_metadata.csv\")\ndf_skin.head()\n\n\n\n    \ndf_skin['path'] = df_skin['image_id'].map(imageid_path_dict.get)\ndf_skin['cell_type'] = df_skin['dx'].map(lesion_type_dict.get) \ndf_skin['cell_type_idx'] = pd.Categorical(df_skin['cell_type']).codes\ndf_skin.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:10.239986Z","iopub.execute_input":"2023-12-04T15:24:10.240700Z","iopub.status.idle":"2023-12-04T15:24:10.353119Z","shell.execute_reply.started":"2023-12-04T15:24:10.240664Z","shell.execute_reply":"2023-12-04T15:24:10.352114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" df_skin['image_id'].map(imageid_path_dict.get)","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:10.354629Z","iopub.execute_input":"2023-12-04T15:24:10.355024Z","iopub.status.idle":"2023-12-04T15:24:10.371867Z","shell.execute_reply.started":"2023-12-04T15:24:10.354976Z","shell.execute_reply":"2023-12-04T15:24:10.370979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\n\ndef quantize_channels(x, k):\n    quantized_x = x.copy()\n    for d in range(3):\n        channel = x[:, :, d].copy()\n        # k_means = MiniBatchKMeans(k, compute_labels=False, n_init='auto')\n        k_means = KMeans(n_clusters=k, random_state=0, n_init='auto')\n        k_means.fit(channel.reshape(-1, 1))\n        labels = k_means.predict(channel.reshape(-1, 1))\n        quantized_x[:,:,d] = labels.reshape(channel.shape)\n    return quantized_x\n\ndef imquantize(x, levels):\n    quantized_img  = quantize_channels(x, levels)\n    M, N, _ = x.shape\n    Index = np.zeros((M,N)).astype('uint8')\n    for i in range(M):\n        for j in range(N):\n            # (0 0 0)base8 = (0*8^2 + 0*8^1 + 0*8^0)base10\n            Index[i][j] = quantized_img[i][j][0] * 64 + quantized_img[i][j][1] * 8 + quantized_img[i][j][2]\n            # Index[i][j] = quantized_img[i][j][0] * (levels*levels) + quantized_img[i][j][1] * levels + quantized_img[i][j][2]\n    return Index\n\n\nfrom scipy.stats import gamma, entropy\nimport numpy as np\nfrom skimage.filters import threshold_otsu\n\ndef fuzzy_divergence_thresholding(gray):\n    with np.errstate(invalid='ignore'):\n        # Compute the Otsu threshold as the initial guess\n        otsu_thresh = threshold_otsu(gray)\n\n        # Define the range of candidate threshold values\n        thresh_range = np.arange(otsu_thresh - 0.1, otsu_thresh + 0.1, 0.01)\n\n        # Compute the fuzzy histogram\n        fuzzy_hist = np.zeros((256, 2))\n        for i in range(256):\n            # Use gamma distribution to compute the membership function\n            # fuzzy_hist[i][0] = gamma.pdf(i, gray.flat[i], scale=20)\n            fuzzy_hist[i][0] = np.exp(-((i - gray.flat[i]) / 20) ** 2)\n            fuzzy_hist[i][1] = 1 - fuzzy_hist[i][0]\n\n        # Compute the divergence between the fuzzy histograms\n        divergence = np.zeros(len(thresh_range))\n        for i, thresh in enumerate(thresh_range):\n            foreground = fuzzy_hist[:int(thresh * 255), 0].sum() + fuzzy_hist[int(thresh * 255):, 1].sum()\n            background = fuzzy_hist[:int(thresh * 255), 1].sum() + fuzzy_hist[int(thresh * 255):, 0].sum()\n            divergence[i] = foreground * np.log(foreground / (foreground + background)) + background * np.log(background / (foreground + background))\n\n        # Find the threshold value that minimizes the divergence\n        # min_div_idx = (np.argmin(divergence)+np.argmax(-divergence))//2\n        min_div_idx = np.argmin(divergence)\n        threshold = thresh_range[min_div_idx]\n\n    return threshold\n\n\nimport numpy as np\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nfrom skimage import color\nfrom skimage import exposure, img_as_float\nfrom skimage.filters import threshold_otsu\nimport cv2\n \nfrom sklearn.preprocessing import MinMaxScaler\n \nfrom sklearn.cluster import KMeans\n\n\ndef segmentation(img):\n    n = 0.5; #distance scaling parameter (n) such that 0.1 ≤ n ≤ 1.0\n    NIC = 3; # number of image components\n    levels = 8; # image quantization level\n    NCD = 512; #levels^3; # maximum number of desired colors\n    LCL = 0; # lab color light\n    LCA = 1; # lab color A\n    LCB = 2; # lab color B\n    PXC = 3; # pixel x-coordinate\n    PYC = 4; # pixel y-coordinate\n    PDC = 5; # pixel distance to image center\n    CPC = 6; # cluster pixel count\n    CPL = 7; # cluster pixel label\n    CCC = 7; # cluster color contrast\n    CSS = 8; # cluster saliency score\n\n    K = 0\n\n    img_arr = np.array(img)\n\n    #---- Image Quantization ----\n    M, N, _ = img_arr.shape\n\n    Index = imquantize(img_arr, levels)\n\n    # Convert image to LAB color space\n    lab_image = color.rgb2lab(img)\n\n    Input = exposure.rescale_intensity(lab_image, out_range=(0, 1))\n\n    Palette = np.zeros((NCD,9))\n\n    for x in range(M):\n        for y in range(N):\n            Palette[Index[x, y], CPC] += 1\n            Palette[Index[x, y], LCL] += Input[x, y, LCL]\n            Palette[Index[x, y], LCA] += Input[x, y, LCA]\n            Palette[Index[x, y], LCB] += Input[x, y, LCB]\n            Palette[Index[x, y], PXC] += x\n            Palette[Index[x, y], PYC] += y\n\n    Cluster = np.zeros((NCD,9))\n    for z in range(NCD):\n        if Palette[z, CPC] > 0:\n            Palette[z, CPL] = K\n            Palette[z, PXC] /= M\n            Palette[z, PYC] /= N\n            Palette[z, 0:PYC+1] /= Palette[z, CPC]\n            Cluster[K, 0:CPC+1] = Palette[z, 0:CPC+1]\n            K += 1\n\n    Cluster = np.delete(Cluster, range(K, NCD), axis=0)\n    Wr = Cluster[:, CPC] / (M * N)\n\n    for x in range(K):\n        Cluster[x, CCC] = 0\n        for y in range(K):\n            Cluster[x, CCC] += Wr[y] * np.linalg.norm(Cluster[x, :NIC] - Cluster[y, :NIC])\n\n    for x in range(M):\n        for y in range(N):\n            cluster_index = int(Palette[Index[x, y], CPL])\n            Cluster[cluster_index, PDC] += ((x / M - 0.5) ** 2) + ((y / N - 0.5) ** 2)\n\n    for z in range(K):\n        Cluster[z, PDC] = Cluster[z, PDC]/(n * n * Cluster[z, CPC])\n\n    for x in range(K):\n        Cluster[x, CSS] = 0\n        for y in range(K):\n            Ds = np.linalg.norm(Cluster[x, PXC:PYC] - Cluster[y, PXC:PYC])\n            Phixy = (Cluster[x, CCC] + 0.05) / (Cluster[y, CCC] + 0.05)\n            Cluster[x, CSS] += Wr[y] * Phixy * np.exp(-Ds)\n        \n        Cluster[x, CSS] = np.exp(-Cluster[x, PDC]) * (Wr[x] * Cluster[x, CCC] + Cluster[x, CSS])\n\n\n    # Cluster[:, CSS] = minmax_scale(Cluster[:, CSS])\n    Cluster[:, CSS] = MinMaxScaler().fit_transform(np.array(Cluster[:, CSS]).reshape(-1, 1)).ravel()\n\n    for x in range(M):\n        for y in range(N):\n            Input[x, y, LCL] = Cluster[int(Palette[Index[x, y], CPL]), CSS]\n\n    # Apply Fuzzy Divergence thresholding to Input\n    # threshold = threshold_otsu(Input[:, :, LCL])\n    threshold = abs(fuzzy_divergence_thresholding(Input[:, :, LCL]))\n    Output = Input[:, :, LCL] >= threshold\n\n    Output = Output.astype(np.uint8) * 255\n\n\n   # assuming that the variable \"Output\" contains the binary image\n    Output = cv2.morphologyEx(Output, cv2.MORPH_CLOSE, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5)))\n\n    # Perform connected component labeling and analysis\n    totalLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(Output, 4, cv2.CV_32S)\n\n    # Find the center of the image\n    h, w = Output.shape\n    cx = w // 2\n    cy = h // 2\n\n    # Compute the distance of each pixel from the center\n    dist = np.sqrt((np.arange(w) - cx) ** 2 + (np.arange(h)[:, None] - cy) ** 2)\n\n    # Find the largest component area\n    max_area = 0\n    for i in range(1, totalLabels):\n        area = stats[i][cv2.CC_STAT_AREA]\n        if area > max_area:\n            max_area = area\n            x, y = centroids[i]\n    \n    min_size = 0.2 * max_area\n    max_dist = dist[int(y), int(x)]\n\n    for i in range(1, totalLabels):\n        area = stats[i][cv2.CC_STAT_AREA]\n        x, y = centroids[i]\n        # Compute the distance of the centroid from the center\n        d = dist[int(y), int(x)]\n        # If the area is smaller than min_size and the distance is larger than max_dist, remove the component\n        if area < min_size and d > max_dist:\n            Output[labels == i] = 0\n\n    # # Convert binary image into an array of (x,y) coordinates\n    # coords = np.column_stack(np.nonzero(Output))\n\n    # # Perform k-means clustering on the coordinates\n    # kmeans = KMeans(n_clusters=2)\n    # kmeans.fit(coords)\n\n    # # Get the cluster labels for each coordinate\n    # labels = kmeans.labels_\n\n    # # Find the cluster with the maximum number of components\n    # largest_cluster = max(set(labels), key=list(labels).count)\n\n    # # Create an output image with the same shape as the input image\n    # out_im = np.zeros_like(Output)\n\n    # # Assign each pixel in the output image to the largest cluster based on its label\n    # for i, coord in enumerate(coords):\n    #     if labels[i] == largest_cluster:\n    #         out_im[coord[0], coord[1]] = 255\n    \n    # Output = out_im\n\n    return Output\n\n\n# #  Load image\n# img = Image.open(\"D:\\\\archive\\HAM10000_images_part_1\\ISIC_0024481.jpg\")\n# output = segmentation(img)\n# output = Image.fromarray(output.astype('uint8'))\n# output.show()\n\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.filters import threshold_otsu\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.color import rgb2hsv\nfrom skimage.filters import gaussian\n\n\ndef blackhat(img):\n    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20,20))\n    hat = cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, rectKernel)\n    return hat\n\ndef tophat(img):\n    rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20,20))\n    hat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, rectKernel)\n    return hat\n\ndef findThreshold(img):\n    fig, ax = plt.subplots(1, 1)\n    y,x,_ = ax.hist(img.ravel(), bins=256, range=[0, 255])\n    x_index = x[np.argmax(y)]\n    y_mean = y.mean()\n    index = int(x_index)\n    for i in range(index, 256):\n        if(y_mean > y[np.where(x==x[i])]):\n            index = i\n            break\n#     ax.axvline(x[index], color='k', linestyle='dashed', linewidth=1)\n#     plt.show()\n    return index\n\n# masking\ndef connected_component_label(img):\n    connectivity = 4\n    numLabels, labels, stats, _ = cv2.connectedComponentsWithStats(img, connectivity, cv2.CV_32S)\n    area = np.mean(stats[1:, cv2.CC_STAT_AREA])\n    mask = np.zeros(img.shape, dtype=\"uint8\")\n    for i in range(1, numLabels):\n        if stats[i, cv2.CC_STAT_AREA] > area:\n            mask[labels == i] = 255\n    maskarea = np.sum(mask) / 255\n    return maskarea, mask\n\ndef removeNestedContours(binary):\n    # Find the contours in the binary image\n    contours, hierarchy = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n\n    # Get the indices of the contours that do not have a parent (i.e. they are not nested)\n    unique_indices = np.where(hierarchy[0][:, 3] == -1)[0]\n\n    # Get the unique contours from the indices\n    unique_contours = [contours[i] for i in unique_indices]\n\n    # creating white mask\n    mask = np.zeros_like(binary)\n    # set all pixels within contours to 255\n    cv2.drawContours(mask, unique_contours, -1, 255, -1)\n    return mask\n\ndef testHoughLines(img):\n    # Convert the image to grayscale\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    # Apply Canny edge detection\n    edges = cv2.Canny(gray, 50, 150, apertureSize=3)\n    # Apply Hough Line Transform to detect hairlines\n    lines = cv2.HoughLinesP(edges, rho=1, theta=np.pi/180, threshold=20, minLineLength=25, maxLineGap=5)\n    if lines is not None:\n        return len(lines)\n    else:\n        return 0\n\ndef process(original_path,filename):#, fldrpath, filename):  # path = filename\n\n    original = cv2.imread(original_path)\n\n    # converting to LAB color space\n    lab = cv2.cvtColor(original, cv2.COLOR_BGR2LAB)\n    l_channel, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l_channel)\n    limg = cv2.merge((cl, a, b))\n\n    # converting back to original\n    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n\n    enhanced_r = enhanced_img[:, :, 2]\n    enhanced_b = enhanced_img[:, :, 0]\n\n    black = blackhat(enhanced_r)\n    ret, threshB = cv2.threshold(black, findThreshold(black), 255, cv2.THRESH_BINARY)\n\n    # filtering\n    hairareaB, filteredB = connected_component_label(threshB)\n\n    # inpaint the original image depending on the mask\n    resultB = cv2.inpaint(original, filteredB, 1, cv2.INPAINT_TELEA)\n\n    # TOPHAT\n    top = tophat(enhanced_b)\n    ret, threshT = cv2.threshold(top, findThreshold(top), 255, cv2.THRESH_BINARY)\n\n    # filtering\n    hairareaT, filteredT = connected_component_label(threshT)\n\n    # inpaint the original image depending on the mask\n    resultT = cv2.inpaint(original, filteredT, 1, cv2.INPAINT_TELEA)\n\n    # segmentation\n    binary_image1 = segmentation(resultB)\n    binary_image2 = segmentation(resultT)\n\n    binary_image1 = removeNestedContours(binary_image1)\n#     cv2.imwrite(f'{fldrpath}\\{filename} Mask B.png', binary_image1)\n    binary_image2 = removeNestedContours(binary_image2)\n#     cv2.imwrite(f'{fldrpath}\\{filename} Mask T.png', binary_image2)\n#     return binary_image1,binary_image2\n\n    white_pixels1 = cv2.countNonZero(binary_image1)\n    white_pixels2 = cv2.countNonZero(binary_image2)\n\n    if(white_pixels1 < white_pixels2):\n        minmask = binary_image1\n    else:\n        minmask = binary_image2\n\n    from pathlib import Path\n    Path('/kaggle/working/mask_image').mkdir(parents=True, exist_ok=True)\n#     Path('/kaggle/working/clear_image').mkdir(parents=True, exist_ok=True)\n#     Path('/kaggle/working/mask_image').mkdir(parents=True, exist_ok=True)\n    \n#     cv2.imwrite(f' Mask_{fldrpath}\\{filename}.png', binary_image2)\n    if testHoughLines(original) > 50:\n        if testHoughLines(resultB) < testHoughLines(resultT):\n#             print(\"a\")\n            masked_img = cv2.bitwise_and(resultB, resultB, mask=minmask)\n            cv2.imwrite(f'/kaggle/working/mask_image/{filename}.png', minmask)\n#             cv2.imwrite(f'/kaggle/working/clear_image/{filename}.png', resultB)\n#             cv2.imwrite(f'/kaggle/working/binary_mask/{filename}.png', masked_img)\n            \n#             return masked_img,minmask,resultB\n        else:\n#             print(\"b\")\n            masked_img = cv2.bitwise_and(resultT, resultT, mask=minmask)\n            cv2.imwrite(f'/kaggle/working/mask_image/{filename}.png', minmask)\n#             cv2.imwrite(f'/kaggle/working/clear_image/{filename}.png', resultT)\n#             cv2.imwrite(f'/kaggle/working/binary_mask/{filename}.png', masked_img)\n            \n#             return masked_img,minmask,resultT\n    else:\n#         print(\"c\")\n        masked_img = cv2.bitwise_and(original, original, mask=minmask)\n        cv2.imwrite(f'/kaggle/working/mask_image/{filename}.png', minmask)\n#         cv2.imwrite(f'/kaggle/working/clear_image/{filename}.png', resultT)\n#         cv2.imwrite(f'/kaggle/working/binary_mask/{filename}.png', masked_img)\n#         return masked_img,minmask,original\n\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:10.373708Z","iopub.execute_input":"2023-12-04T15:24:10.374228Z","iopub.status.idle":"2023-12-04T15:24:11.064248Z","shell.execute_reply.started":"2023-12-04T15:24:10.374194Z","shell.execute_reply":"2023-12-04T15:24:11.062878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df=df_skin.sample(2)\npaths=df_skin[8000:].path.values\nimage_ids=df_skin[8000:].image_id.values\n\n\ndel df_skin\n\nlen(paths)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:11.066779Z","iopub.execute_input":"2023-12-04T15:24:11.067786Z","iopub.status.idle":"2023-12-04T15:24:11.078143Z","shell.execute_reply.started":"2023-12-04T15:24:11.067742Z","shell.execute_reply":"2023-12-04T15:24:11.077100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index, image_path in tqdm(enumerate(paths)):\n#     print(f\"Image ID: {image_id}, Image Path: {image_path}\")\n    process(image_path,image_ids[index])","metadata":{"execution":{"iopub.status.busy":"2023-12-04T15:24:11.079564Z","iopub.execute_input":"2023-12-04T15:24:11.080185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}